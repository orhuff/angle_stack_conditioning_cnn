{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f222e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import segyio\n",
    "from shutil import copyfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b381b58",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1bbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segy copying and writing function\n",
    "def block_write_mmap(output_segy, input_segy, data_cube, mmap=True):\n",
    "    copyfile(input_segy, output_segy)\n",
    "\n",
    "    with segyio.open(output_segy, \"r\") as segy_src:\n",
    "        n_xl = len(segy_src.xlines)\n",
    "        n_il = len(segy_src.ilines)\n",
    "        n_smpl = len(segy_src.samples)\n",
    "        expected_shape = (n_il, n_xl, n_smpl)\n",
    "    with segyio.open(output_segy, \"r+\", ignore_geometry=True) as segy_dst:\n",
    "        if mmap:\n",
    "            mapped_success = segy_dst.mmap()\n",
    "        if data_cube.shape != expected_shape:\n",
    "            raise ValueError(\n",
    "                f\"dataset has shape {data_cube.shape} which is not the expected shape {expected_shape}\")\n",
    "        data_cube = np.ascontiguousarray(data_cube, 'float32')\n",
    "        segy_dst.trace.raw[:] = data_cube.reshape((n_xl * n_il, n_smpl))\n",
    "        print('Done writing ' + output_segy)\n",
    "        return mapped_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full loading, prediction, and writing function\n",
    "def condition_angle_stacks(model_h5, t_max, near_inp_segy, mid_inp_segy, far_inp_segy, near_out_segy, mid_out_segy, far_out_segy):\n",
    "    \n",
    "    # load original data as numpy arrays\n",
    "    near_orig = segyio.tools.cube(near_inp_segy)\n",
    "    mid_orig = segyio.tools.cube(mid_inp_segy)\n",
    "    far_orig = segyio.tools.cube(far_inp_segy)\n",
    "    \n",
    "    # load trained model\n",
    "    model = tf.keras.models.load_model(model_h5, compile=False)\n",
    "\n",
    "    # retain original time length of data\n",
    "    t_orig = near_orig.shape[2]\n",
    "    \n",
    "    # if the original seismic data has fewer time samples than the tmax of the model\n",
    "    if t_orig < t_max:\n",
    "        \n",
    "        # pad the data with zeros at the end to make it the same length tmax\n",
    "        zeros_to_add_before = t_max - t_orig\n",
    "        zero_array_before = np.zeros((near_orig.shape[0], near_orig.shape[1], zeros_to_add_before))\n",
    "        near_orig = np.concatenate((near_orig, zero_array_before), axis=2)\n",
    "        mid_orig = np.concatenate((mid_orig, zero_array_before), axis=2)\n",
    "        far_orig = np.concatenate((far_orig, zero_array_before), axis=2)\n",
    "        \n",
    "    # if the original data has more samples (or the same) as tmax\n",
    "    else:\n",
    "\n",
    "        near_orig = near_orig[:, :, :t_max]\n",
    "        mid_orig = mid_orig[:, :, :t_max]\n",
    "        far_orig = far_orig[:, :, :t_max]\n",
    "\n",
    "    # initialize arrays for cnn prediction\n",
    "    near_cnn = np.zeros(near_orig.shape)\n",
    "    mid_cnn = np.zeros(mid_orig.shape)\n",
    "    far_cnn = np.zeros(far_orig.shape)\n",
    "\n",
    "    # iterate over crossline number\n",
    "    for xline in range(near_orig.shape[1]):\n",
    "\n",
    "        # iterate over inline number\n",
    "        counter = 1\n",
    "        for inline in range(near_orig.shape[0]):\n",
    "\n",
    "            # combine near,mid,far into stacked array, and reshape for tensorflow format\n",
    "            orig_stacks = np.stack((near_orig[inline, xline, :], mid_orig[inline, xline, :], far_orig[inline, xline, :]), axis=1)\n",
    "            orig_stacks = np.reshape(orig_stacks, (1, t_max, 3, 1))\n",
    "\n",
    "            # make prediction with model, and extract it from tensorflow format\n",
    "            pred_stacks = model.predict(orig_stacks, verbose=False)\n",
    "            pred_stacks = pred_stacks[0,:,:]      \n",
    "\n",
    "            # fill in cnn prediction arrays\n",
    "            near_cnn[inline, xline, :] = pred_stacks[:,0]\n",
    "            mid_cnn[inline, xline, :] = pred_stacks[:,1]\n",
    "            far_cnn[inline, xline, :] = pred_stacks[:,2]\n",
    "\n",
    "            # print out progress every 400 CDP locations\n",
    "            if counter % 400 == 0:\n",
    "                print('XL = '+str(xline+1)+' / '+str(near_orig.shape[1])+'  |  IL = '+ str(inline+1)+' / '+str(near_orig.shape[0]))\n",
    "            counter += 1\n",
    "            \n",
    "    # normalize CNN-predicted amplitudes to same level as average of original amplitudes\n",
    "    norm_near = np.mean(np.abs(near_orig)) / np.mean(np.abs(near_cnn))\n",
    "    norm_mid = np.mean(np.abs(mid_orig)) / np.mean(np.abs(mid_cnn))\n",
    "    norm_far = np.mean(np.abs(far_orig)) / np.mean(np.abs(far_cnn))\n",
    "\n",
    "    near_cnn *= norm_near\n",
    "    mid_cnn *= norm_mid\n",
    "    far_cnn *= norm_far\n",
    "    \n",
    "    print(' ')\n",
    "    print('DONE MAKING PREDICTIONS')\n",
    "    print(' ')\n",
    "\n",
    "    # if the original seismic data had more time samples than the tmax of the model\n",
    "    if t_orig > t_max:\n",
    "        \n",
    "        # calculate number of zeros to add back to array, for writing back to initial segy geometry\n",
    "        zeros_to_add_after = t_orig - t_max\n",
    "\n",
    "        # pad the prediction arrays past 1248 samples with zeros\n",
    "        zero_array_after = np.zeros((near_orig.shape[0], near_orig.shape[1], zeros_to_add_after))\n",
    "        near_cnn_pad = np.concatenate((near_cnn, zero_array_after), axis=2)\n",
    "        mid_cnn_pad = np.concatenate((mid_cnn, zero_array_after), axis=2)\n",
    "        far_cnn_pad = np.concatenate((far_cnn, zero_array_after), axis=2)\n",
    "        \n",
    "    # if the original seismic data had fewer time samples (or the same) than the tmax of the model    \n",
    "    else:\n",
    "        \n",
    "        near_cnn_pad = near_cnn[:, :, :t_orig]\n",
    "        mid_cnn_pad = mid_cnn[:, :, :t_orig]\n",
    "        far_cnn_pad = far_cnn[:, :, :t_orig]\n",
    "        \n",
    "    # write near, mid, and far segy prediction files\n",
    "    block_write_mmap(near_out_segy, near_inp_segy, near_cnn_pad, mmap=True)\n",
    "    block_write_mmap(mid_out_segy, mid_inp_segy, mid_cnn_pad, mmap=True)\n",
    "    block_write_mmap(far_out_segy, far_inp_segy, far_cnn_pad, mmap=True)\n",
    "    print('')\n",
    "    \n",
    "    return 'ALL FILES WRITTEN'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22514b3f",
   "metadata": {},
   "source": [
    "# Run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9788d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XL = 1 / 3  |  IL = 400 / 3506\n",
      "XL = 1 / 3  |  IL = 800 / 3506\n",
      "XL = 1 / 3  |  IL = 1200 / 3506\n",
      "XL = 1 / 3  |  IL = 1600 / 3506\n",
      "XL = 1 / 3  |  IL = 2000 / 3506\n",
      "XL = 1 / 3  |  IL = 2400 / 3506\n",
      "XL = 1 / 3  |  IL = 2800 / 3506\n",
      "XL = 1 / 3  |  IL = 3200 / 3506\n",
      "XL = 2 / 3  |  IL = 400 / 3506\n",
      "XL = 2 / 3  |  IL = 800 / 3506\n",
      "XL = 2 / 3  |  IL = 1200 / 3506\n",
      "XL = 2 / 3  |  IL = 1600 / 3506\n",
      "XL = 2 / 3  |  IL = 2000 / 3506\n",
      "XL = 2 / 3  |  IL = 2400 / 3506\n",
      "XL = 2 / 3  |  IL = 2800 / 3506\n",
      "XL = 2 / 3  |  IL = 3200 / 3506\n",
      "XL = 3 / 3  |  IL = 400 / 3506\n",
      "XL = 3 / 3  |  IL = 800 / 3506\n",
      "XL = 3 / 3  |  IL = 1200 / 3506\n",
      "XL = 3 / 3  |  IL = 1600 / 3506\n",
      "XL = 3 / 3  |  IL = 2000 / 3506\n",
      "XL = 3 / 3  |  IL = 2400 / 3506\n",
      "XL = 3 / 3  |  IL = 2800 / 3506\n",
      "XL = 3 / 3  |  IL = 3200 / 3506\n",
      " \n",
      "DONE MAKING PREDICTIONS\n",
      " \n",
      "Done writing ./pgs15917/PGS15917VIK_near_line3_CNN.sgy\n",
      "Done writing ./pgs15917/PGS15917VIK_mid_line3_CNN.sgy\n",
      "Done writing ./pgs15917/PGS15917VIK_far_line3_CNN.sgy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ALL FILES WRITTEN'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_max = 1248\n",
    "\n",
    "# set these as the paths to the model file, the near/mid/far input segys, and desired name/path for writing results\n",
    "model_h5 = 'unet-resnet-102030.h5'\n",
    "\n",
    "near_inp_segy = 'PGS15917VIK_near_cube.sgy'\n",
    "mid_inp_segy = 'PGS15917VIK_mid_cube.sgy'\n",
    "far_inp_segy = 'PGS15917VIK_far_cube.sgy'\n",
    "\n",
    "near_out_segy = 'PGS15917VIK_near_cube_CNN.sgy'\n",
    "mid_out_segy = 'PGS15917VIK_mid_cube_CNN.sgy'\n",
    "far_out_segy = 'PGS15917VIK_far_cube_CNN.sgy'\n",
    "\n",
    "# call function\n",
    "condition_angle_stacks(model_h5, t_max, near_inp_segy, mid_inp_segy, far_inp_segy, near_out_segy, mid_out_segy, far_out_segy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1178a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
